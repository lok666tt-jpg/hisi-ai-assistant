import streamlit as st
from openai import OpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# ================= 1. UI çš®å›Šé…ç½® =================
st.set_page_config(page_title="HiSi ç™½çš®ä¹¦ä¸“å®¶", page_icon="ğŸ“–")
st.title("ğŸ“– HiSi-G.I.D.S. ç™½çš®ä¹¦æ™ºèƒ½æ£€ç´¢")
st.caption("å·²æŒ‚è½½æœ¬åœ° PDF çŸ¥è¯†åº“ï¼ŒåŸºäºä¼ä¸šçº§ç‰©ç†ç¡¬ç›˜ç§’çº§æ£€ç´¢")

# ================= 2. æ ¸å¿ƒæ¶æ„å˜æ›´ (PM è¯·æ³¨æ„ï¼) =================
@st.cache_resource
def load_local_knowledge_base():
    # 1. è¯·å‡ºç¿»è¯‘å®˜
    embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5")
    
    # 2. ğŸ’¡ é­”æ³•å˜æ›´ï¼šä¸è¦å†ç”¨ from_texts ä»å¤´å»ºäº†ï¼
    # ç›´æ¥å‘Šè¯‰ç¨‹åºï¼šå»è¯»å–ç°æˆçš„ hisi_vdb æ–‡ä»¶å¤¹
    db = Chroma(persist_directory="hisi_vdb", embedding_function=embedding_model)
    return db

# æŒ‚è½½â€œè®°å¿† U ç›˜â€å¹¶å”¤é†’å¤§æ¨¡å‹
db = load_local_knowledge_base()
client = OpenAI(
    # ğŸ’¡ æ ¸å¿ƒå®‰å…¨å˜æ›´ï¼šä¸å†å†™æ­»ç§˜é’¥ï¼
    # å‘Šè¯‰ç¨‹åºï¼šâ€œç­‰ä¸Šäº†äº‘æœåŠ¡å™¨ï¼Œå»æœåŠ¡å™¨çš„å®‰å…¨ä¿é™©ç®±(secrets)é‡Œæ‹¿ç§˜é’¥â€
    api_key=st.secrets["DEEPSEEK_API_KEY"],  
    base_url="https://api.deepseek.com"
)

# ================= 3. ä¼šè¯è®°å¿†åˆå§‹åŒ– =================
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ================= 4. æ£€ç´¢æµæ°´çº¿ =================
user_input = st.chat_input("å…³äºã€Šäº§å“ç™½çš®ä¹¦ã€‹ï¼Œæ‚¨æƒ³æŸ¥é˜…ä»€ä¹ˆæ ¸å¿ƒåŠŸèƒ½æˆ–æ¡æ¬¾ï¼Ÿ")

if user_input:
    # A. è®°å½•é—®é¢˜
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    # B. å¼€å§‹åœ¨å¤šç»´å®‡å®™ä¸­å¯»æ‰¾ç­”æ¡ˆ
    with st.spinner("ğŸ” æ­£åœ¨ç§’çº§æ£€ç´¢ã€Šäº§å“ç™½çš®ä¹¦ã€‹æ ¸å¿ƒæ¡æ¬¾..."):
        
        # ğŸ’¡ ç­–ç•¥è°ƒä¼˜ï¼šå› ä¸º PDF å†…å®¹å¤šï¼Œæˆ‘ä»¬æŠŠ k=1 æ”¹æˆ k=3
        # ä¹Ÿå°±æ˜¯ä¸€æ¬¡æ€§æŠ“å– 3 ä¸ªæœ€ç›¸å…³çš„æ®µè½ï¼Œç»™å¤§æ¨¡å‹æ›´å……è¶³çš„ä¸Šä¸‹æ–‡
        results = db.similarity_search(user_input, k=3)
        
        # æŠŠæ‰¾åˆ°çš„ 3 ä¸ªæ®µè½æ‹¼è£…èµ·æ¥
        retrieved_knowledge = ""
        for i, res in enumerate(results):
            retrieved_knowledge += f"ã€å‚è€ƒæ®µè½ {i+1}ã€‘:\n{res.page_content}\n\n"
            
        # å¼ºè¡Œæ³¨å…¥ç³»ç»Ÿæç¤ºè¯
        system_prompt = f"""
        ä½ æ˜¯ HiSi-G.I.D.S. ç³»ç»Ÿçš„èµ„æ·±äº§å“ä¸“å®¶ã€‚
        è¯·ã€ä¸¥æ ¼åŸºäºä»¥ä¸‹ç™½çš®ä¹¦æå–çš„å†…å®¹ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœä¸‹é¢æä¾›çš„å†…å®¹é‡Œæ²¡å†™ï¼Œè¯·è¯šå®åœ°å›ç­”â€œç™½çš®ä¹¦ä¸­æœªæåŠæ­¤ä¿¡æ¯â€ï¼Œç¦æ­¢çç¼–ï¼
        
        ã€ç™½çš®ä¹¦æå–å†…å®¹ã€‘ï¼š
        {retrieved_knowledge}
        """
        
        api_messages = [{"role": "system", "content": system_prompt}] + st.session_state.messages
        
        # C. ç”Ÿæˆå›ç­”
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=api_messages
        )
        ai_reply = response.choices[0].message.content
        
    # D. å±•ç¤ºå›ç­”
    with st.chat_message("assistant"):
        st.markdown(ai_reply)
    st.session_state.messages.append({"role": "assistant", "content": ai_reply})
import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever
from openai import OpenAI
import pickle
import jieba

st.set_page_config(page_title="HiSi & ç»´ä¿ æ™ºèƒ½è°ƒåº¦å¤§è„‘", page_icon="âœˆï¸")
st.title("âœˆï¸ ä¼ä¸šçº§å¤šè·¯ç”±ä¸šåŠ¡å¤§è„‘ (å®Œå…¨ä½“)")
st.caption("å·²æŒ‚è½½ï¼šè°ƒåº¦ Agent + èˆªæ˜¾åŒå¼•æ“ + ç»´ä¿å‘é‡åº“ + è¿ç»­è®°å¿†ã€‚")

# ================= 1. æŒ‚è½½æ‰€æœ‰ U ç›˜ä¸å¼•æ“ =================
@st.cache_resource
def load_all_engines():
    embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5")
    
    # --- æŠ½å±‰ Aï¼šèˆªæ˜¾ç³»ç»Ÿ (åŒå¼•æ“æ»¡é…ç‰ˆ) ---
    hisi_vector_db = Chroma(persist_directory="hisi_vdb", embedding_function=embedding_model)
    hisi_vector_retriever = hisi_vector_db.as_retriever(search_kwargs={"k": 3})
    
    with open("hisi_chunks.pkl", "rb") as f:
        hisi_chunks = pickle.load(f)
    def jieba_tokenizer(text): return list(jieba.cut(text))
    hisi_bm25_retriever = BM25Retriever.from_texts(hisi_chunks, preprocess_func=jieba_tokenizer)
    hisi_bm25_retriever.k = 3
    
    hisi_ensemble = EnsembleRetriever(
        retrievers=[hisi_bm25_retriever, hisi_vector_retriever],
        weights=[0.5, 0.5]
    )
    
    # --- æŠ½å±‰ Bï¼šè½¦è¾†ä¸æ¡¥è½½è®¾å¤‡ç»´ä¿ç³»ç»Ÿ (çº¯å‘é‡ç‰ˆ) ---
    bridge_vector_db = Chroma(persist_directory="bridge_vdb", embedding_function=embedding_model)
    bridge_retriever = bridge_vector_db.as_retriever(search_kwargs={"k": 3})
    
    return hisi_ensemble, bridge_retriever

hisi_retriever, bridge_retriever = load_all_engines()

client = OpenAI(
    api_key=st.secrets["DEEPSEEK_API_KEY"], 
    base_url="https://api.deepseek.com"
)

# ================= 2. ç½‘é¡µè®°äº‹æœ¬ =================
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ================= 3. æ ¸å¿ƒè°ƒåº¦ä¸æ£€ç´¢æµæ°´çº¿ =================
def classify_intent(question):
prompt = f"""ä½ æ˜¯ä¸€ä¸ªæå…¶èªæ˜çš„æœºåœºä¸šåŠ¡æ€»è°ƒåº¦å‘˜ã€‚
    è¯·åˆ¤æ–­ä¸‹é¢è¿™ä¸ªé—®é¢˜ï¼Œå±äºå“ªä¸ªä¸šåŠ¡é¢†åŸŸï¼š
    A: èˆªæ˜¾ç³»ç»Ÿã€å±å¹•å‚æ•°ã€è½¯ä»¶åŠŸèƒ½ (HiSi-G.I.D.S)
    B: è½¦è¾†ç»´ä¿®ã€ç™»æœºæ¡¥ã€ç†èµ”ã€æ´—è½¦ã€å·¥æ—¶åˆ†é…ã€æ¡¥è½½è®¾å¤‡ã€æ–½å·¥ç®¡ç†ã€æ–½å·¥ç°åœºã€ç»´ä¿
    
    ä½ åªèƒ½å›ç­”ä¸€ä¸ªå¤§å†™å­—æ¯ 'A' æˆ– 'B'ï¼Œç»å¯¹ä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–æ ‡ç‚¹æˆ–åºŸè¯ã€‚
    ç”¨æˆ·é—®é¢˜ï¼š{question}"""
    
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    return response.choices[0].message.content.strip()

if prompt := st.chat_input("å°è¯•è·¨ç•Œæ‹·é—®ï¼ˆå¦‚ï¼šå…ˆé—®å±å¹•å‹å·ï¼Œå†é—®æ´—è½¦å·¥æ—¶ï¼‰..."):
    
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        # ğŸ’¡ æ­¥éª¤ 1ï¼šå‘¼å«å¤§å ‚è°ƒåº¦å‘˜è¿›è¡Œåˆ†å‘ï¼
        intent = classify_intent(prompt)
        
        # ğŸ’¡ æ­¥éª¤ 2ï¼šæ ¹æ®åˆ†ç±»å»ä¸åŒçš„æŠ½å±‰æ‹¿èµ„æ–™ï¼Œå¹¶åœ¨ç½‘é¡µä¸Šå®æ—¶æ’­æŠ¥ï¼
        if "A" in intent:
            st.caption("ğŸ¤– *è°ƒåº¦å‘˜æ€è€ƒä¸­... -> é”å®šä¸ºã€èˆªæ˜¾ä¸šåŠ¡ã€‘ï¼Œå·²å‰å¾€æŠ½å±‰ A (å¯åŠ¨åŒå¼•æ“æ£€ç´¢)*")
            docs = hisi_retriever.invoke(prompt)
        elif "B" in intent:
            st.caption("ğŸ¤– *è°ƒåº¦å‘˜æ€è€ƒä¸­... -> é”å®šä¸ºã€ç»´ä¿ä¸šåŠ¡ã€‘ï¼Œå·²å‰å¾€æŠ½å±‰ B (å¯åŠ¨ä¸“æœ‰çŸ¥è¯†åº“)*")
            docs = bridge_retriever.invoke(prompt)
        else:
            st.caption("ğŸ¤– *è°ƒåº¦å‘˜é‡åˆ°æœªçŸ¥é¢†åŸŸï¼Œé»˜è®¤å‰å¾€æŠ½å±‰ A ç¢°ç¢°è¿æ°”...*")
            docs = hisi_retriever.invoke(prompt)
            
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # ğŸ’¡ æ­¥éª¤ 3ï¼šå¸¦ç€ç‰¹å®šæŠ½å±‰çš„èµ„æ–™ï¼Œç»“åˆå†å²è®°å¿†ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”
        api_messages = [
            {"role": "system", "content": "ä½ æ˜¯æå…¶ä¸¥è°¨çš„ä¸šåŠ¡AIä¸“å®¶ã€‚è¯·ç»“åˆæˆ‘æä¾›çš„ã€å‚è€ƒçŸ¥è¯†åº“ã€‘å’Œã€å†å²å¯¹è¯è®°å½•ã€‘æ¥å›ç­”æœ€æ–°é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“æœªæåŠï¼Œç›´æ¥å›ç­”ä¸çŸ¥é“ã€‚"}
        ]
        
        for msg in st.session_state.messages[:-1]:
            api_messages.append({"role": msg["role"], "content": msg["content"]})
            
        latest_prompt_with_context = f"ã€å‚è€ƒçŸ¥è¯†åº“ã€‘\n{context}\n\nã€æœ€æ–°é—®é¢˜ã€‘\n{prompt}"
        api_messages.append({"role": "user", "content": latest_prompt_with_context})
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=api_messages,
            stream=False
        )
        
        answer = response.choices[0].message.content
        st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})






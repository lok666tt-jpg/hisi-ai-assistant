import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever
from openai import OpenAI
import pickle
import jieba

st.set_page_config(page_title="HiSi æ™ºèƒ½ä¸šåŠ¡å¤§è„‘", page_icon="âœˆï¸")
st.title("âœˆï¸ HiSi-G.I.D.S. V2.0 (åŒå¼•æ“è®°å¿†ç‰ˆ)")
st.caption("å·²æŒ‚è½½ï¼šè¯­ä¹‰å‘é‡å¼•æ“ + BM25 ç²¾ç¡®åŒ¹é…å¼•æ“ã€‚éšä¾¿æ‹·é—®æåº¦ç”Ÿåƒ»çš„è®¾å¤‡å‹å·ï¼")

# ================= 1. æŒ‚è½½åŒå¼•æ“ =================
@st.cache_resource
def load_dual_engines():
    print("â³ æ­£åœ¨å¯åŠ¨æ–‡ç§‘ç”Ÿï¼šè¯­ä¹‰å‘é‡å¼•æ“...")
    embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5")
    vector_db = Chroma(persist_directory="hisi_vdb", embedding_function=embedding_model)
    # åŒ…è£…æˆæ£€ç´¢å™¨
    vector_retriever = vector_db.as_retriever(search_kwargs={"k": 3})
    
    print("â³ æ­£åœ¨å¯åŠ¨ç†ç§‘ç”Ÿï¼šBM25 ç²¾ç¡®åŒ¹é…å¼•æ“...")
    # æŠŠä¹‹å‰æŠ½çœŸç©ºçš„å­—å…¸è§£å†»æ‹¿å‡ºæ¥
    with open("hisi_chunks.pkl", "rb") as f:
        chunks = pickle.load(f)
        
    # ğŸ’¡ æ ¸å¿ƒï¼šæ•™ BM25 æ€ä¹ˆåˆ‡åˆ†ä¸­æ–‡è¯è¯­
    def jieba_tokenizer(text):
        return list(jieba.cut(text))
        
    bm25_retriever = BM25Retriever.from_texts(chunks, preprocess_func=jieba_tokenizer)
    bm25_retriever.k = 3
    
    print("ğŸ¤ æ­£åœ¨èåˆï¼šå¤§å ‚ç»ç†å°±ä½...")
    # æƒé‡å„å  50%ï¼Œä½ å¯ä»¥æ ¹æ®ä¸šåŠ¡éœ€è¦éšæ—¶è°ƒé…ï¼Œæ¯”å¦‚ç²¾ç¡®åŒ¹é…è¦æ±‚é«˜ï¼Œå¯ä»¥æ”¹æˆ [0.7, 0.3]
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[0.5, 0.5]
    )
    return ensemble_retriever

# è·å–é›†æˆæ£€ç´¢å™¨
ensemble_retriever = load_dual_engines()

client = OpenAI(
    api_key=st.secrets["DEEPSEEK_API_KEY"], 
    base_url="https://api.deepseek.com"
)

# ================= 2. ç½‘é¡µè®°äº‹æœ¬ (ä¿æŒä¸å˜) =================
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ================= 3. å¸¦è®°å¿†çš„äº¤äº’æµæ°´çº¿ =================
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆè¯•ç€æœä¸€ä¸ªæå…¶ç²¾ç¡®çš„å‹å·ï¼‰..."):
    
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        # ğŸ’¡ æ ¸å¿ƒå˜åŒ–ï¼šä»¥å‰æ˜¯ db.similarity_searchï¼Œç°åœ¨ç›´æ¥å‘¼å«å¤§å ‚ç»ç†ï¼
        docs = ensemble_retriever.invoke(prompt)
        context = "\n\n".join([doc.page_content for doc in docs])
        
        api_messages = [
            {"role": "system", "content": "ä½ æ˜¯æå…¶ä¸¥è°¨çš„æœºåœºä¸šåŠ¡AIä¸“å®¶ã€‚è¯·ç»“åˆæˆ‘æä¾›çš„ã€å‚è€ƒçŸ¥è¯†åº“ã€‘å’Œã€å†å²å¯¹è¯è®°å½•ã€‘æ¥å›ç­”æœ€æ–°é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“æœªæåŠï¼Œç›´æ¥å›ç­”ä¸çŸ¥é“ã€‚"}
        ]
        
        for msg in st.session_state.messages[:-1]:
            api_messages.append({"role": msg["role"], "content": msg["content"]})
            
        latest_prompt_with_context = f"ã€å‚è€ƒçŸ¥è¯†åº“ã€‘\n{context}\n\nã€æœ€æ–°é—®é¢˜ã€‘\n{prompt}"
        api_messages.append({"role": "user", "content": latest_prompt_with_context})
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=api_messages,
            stream=False
        )
        
        answer = response.choices[0].message.content
        st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})



